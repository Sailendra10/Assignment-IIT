{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMFCsdV0cdDj35jvdkzi2u9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Assignment 1: Implementing Document Loaders in LangChain\n","\n","## Objective:\n","Write a Python script that uses LangChain’s document loaders to load documents from a directory. Your task is to implement functionality that reads `.txt` and `.pdf` files and outputs their content as LangChain `Document` objects.\n","\n","---\n","\n","## Requirements:\n","1. Use LangChain’s `TextLoader` for `.txt` files and `PyPDFLoader` for `.pdf` files.\n","2. Implement a function `load_documents(directory: str)` that:\n","   - Iterates through all files in the specified directory.\n","   - Loads the content of `.txt` and `.pdf` files.\n","   - Returns a list of `Document` objects, where:\n","     - **Page Content:** Contains the file’s text content.\n","     - **Metadata:** Includes the filename or other relevant file metadata.\n","3. Handle unsupported file types or errors gracefully.\n","\n","---\n","\n","## Input:\n","- A directory containing files with extensions `.txt` and `.pdf`.\n","\n","---\n","\n","## Output:\n","- A list of LangChain `Document` objects. Each document should contain:\n","  - The text content of the file.\n","  - Metadata such as the filename.\n","\n","---\n","\n","## Example:\n","### Input:\n","A directory with the following files:\n","- `example.txt` containing \"Hello, this is a text file.\"\n","- `example.pdf` containing \"This is a PDF document.\"\n","- `image.jpg` (unsupported file type).\n","\n","### Output:\n","```python\n","[\n","    Document(page_content=\"Hello, this is a text file.\", metadata={\"filename\": \"example.txt\"}),\n","    Document(page_content=\"This is a PDF document.\", metadata={\"filename\": \"example.pdf\"})\n","]"],"metadata":{"id":"HLU10QfSWXC6"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"w0DnO8NYU38U"},"outputs":[],"source":["from langchain.document_loaders import TextLoader, PyPDFLoader\n","from langchain.schema import Document\n","import os\n","from typing import List\n","\n","def load_documents(directory: str) -> List[Document]:\n","    \"\"\"\n","    Skeleton Function: Load .txt and .pdf documents from a directory.\n","\n","    Args:\n","        directory (str): Path to the directory containing files.\n","\n","    Returns:\n","        List[Document]: A list of LangChain Document objects.\n","    \"\"\"\n","    # Initialize an empty list to store the documents\n","    documents = []\n","\n","    # Loop through files in the specified directory\n","    for filename in os.listdir(directory):\n","        file_path = os.path.join(directory, filename)\n","\n","        try:\n","            # Placeholder for loading .txt files\n","            if filename.endswith(\".txt\"):\n","                pass  # Replace with code for TextLoader\n","\n","            # Placeholder for loading .pdf files\n","            elif filename.endswith(\".pdf\"):\n","                pass  # Replace with code for PyPDFLoader\n","\n","        except Exception as e:\n","            # Print error for files that could not be loaded\n","            print(f\"Error loading {filename}: {e}\")\n","\n","    # Return the list of documents\n","    return documents\n","\n","# Example usage\n","if __name__ == \"__main__\":\n","    # Define the directory path\n","    directory_path = \"/path/to/your/files\"\n","\n","    # Call the function to load documents\n","    docs = load_documents(directory_path)\n","\n","    # Iterate through the loaded documents and print metadata and content preview\n","    for doc in docs:\n","        print(f\"File: {doc.metadata.get('filename', 'Unknown')}, Content Preview: {doc.page_content[:100]}\")"]},{"cell_type":"markdown","source":["# Assignment: Chunking Data and Converting It to Vector Embeddings\n","\n","## Objective:\n","Write a Python script that uses LangChain to:\n","1. Load `.txt` and `.pdf` files as `Document` objects from a directory.\n","2. Chunk the data into smaller pieces for efficient processing.\n","3. Convert the chunks into vector embeddings using a text embedding model.\n","\n","---\n","\n","## Requirements:\n","1. **Document Loading**:\n","   - Use LangChain’s `TextLoader` for `.txt` files and `PyPDFLoader` for `.pdf` files.\n","   - Implement a function `load_documents(directory: str)` to load all files from a directory as LangChain `Document` objects.\n","\n","2. **Chunking**:\n","   - Use LangChain’s `RecursiveCharacterTextSplitter` to split the document text into smaller chunks.\n","   - Implement a function `chunk_documents(documents: List[Document]) -> List[Document]`.\n","\n","3. **Embedding Generation**:\n","   - Use a pre-trained embedding model (e.g., `OpenAIEmbeddings` or any other LangChain-compatible embedding model).\n","   - Implement a function `generate_embeddings(chunks: List[Document]) -> List[List[float]]` that converts each chunk into a vector embedding.\n","\n","4. **Error Handling**:\n","   - Handle unsupported file types and errors gracefully.\n","\n","---\n","\n","## Input:\n","- A directory containing `.txt` and `.pdf` files.\n","\n","---\n","\n","## Output:\n","- A list of vector embeddings for the chunks of the loaded documents.\n","\n","---\n","\n","## Example:\n","### Input:\n","A directory with the following files:\n","- `example.txt` containing \"This is an example text file.\"\n","- `example.pdf` containing \"This is an example PDF document.\"\n","\n","### Output:\n","A list of embeddings (e.g., 768-dimensional vectors) for the chunks generated from the documents.\n","\n","---"],"metadata":{"id":"b_YBadNLWsho"}},{"cell_type":"code","source":["from langchain.schema import Document\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.embeddings import OpenAIEmbeddings\n","from typing import List\n","\n","def chunk_documents(documents: List[Document]) -> List[Document]:\n","    \"\"\"\n","    Splits documents into smaller chunks.\n","\n","    Args:\n","        documents (List[Document]): List of LangChain Document objects.\n","\n","    Returns:\n","        List[Document]: A list of chunked Document objects.\n","    \"\"\"\n","    # Create an instance of the text splitter with specified chunk size and overlap\n","    text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)\n","    chunks = []\n","\n","    # Iterate over each document and split it into chunks\n","    for doc in documents:\n","        # Split the document and add chunks to the list\n","        chunks.extend(text_splitter.split_documents([doc]))\n","\n","    return chunks\n","\n","def generate_embeddings(chunks: List[Document]) -> List[List[float]]:\n","    \"\"\"\n","    Generates vector embeddings for the given chunks.\n","\n","    Args:\n","        chunks (List[Document]): List of chunked Document objects.\n","\n","    Returns:\n","        List[List[float]]: A list of vector embeddings.\n","    \"\"\"\n","    # Initialize the OpenAI embeddings model\n","    embeddings = OpenAIEmbeddings()\n","\n","    # Generate embeddings for each chunk\n","    return [embeddings.embed_query(chunk.page_content) for chunk in chunks]\n","\n","# Example Usage\n","if __name__ == \"__main__\":\n","    # Sample directory path where documents are stored\n","    directory_path = \"/path/to/directory\"\n","\n","    # Load documents (This function is implemented in Assignment 1)\n","    documents = load_documents(directory_path)\n","\n","    # Chunk the documents into smaller chunks\n","    chunks = chunk_documents(documents)\n","\n","    # Generate embeddings for the chunks\n","    embeddings = generate_embeddings(chunks)\n","\n","    # Display first 5 embeddings for demonstration\n","    for i, embedding in enumerate(embeddings[:5]):  # Display first 5 embeddings for brevity\n","        print(f\"Embedding {i + 1}: {embedding[:10]}...\")  # Print first 10 dimensions for brevity"],"metadata":{"id":"pXB9pEuNXS_f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Assignment 3: Creating and Querying a Vector Database with Chroma\n","\n","## Objective:\n","Write a Python script to:\n","1. Create a vector database using the **Chroma** library.\n","2. Store vector embeddings of document chunks in the database.\n","3. Query the database using similarity search and retrieve the top `k` results.\n","\n","---\n","\n","## Requirements:\n","1. **Vector Database Creation**:\n","   - Use Chroma to create a persistent vector database.\n","   - Add document embeddings (e.g., from OpenAI or any other embedding model) along with metadata to the database.\n","\n","2. **Similarity Search**:\n","   - Implement a function to query the database with a user-provided text and retrieve the top `k` most similar results.\n","\n","3. **Input Data**:\n","   - Use a list of text chunks or embeddings for this task. You may generate these from documents (e.g., `.txt` or `.pdf` files).\n","\n","4. **Outputs**:\n","   - Return the metadata and content of the top `k` most similar results from the database.\n","\n","---\n","\n","## Example:\n","### Input:\n","1. A collection of text chunks from documents such as:\n","   - `\"LangChain is a framework for developing applications powered by LLMs.\"`\n","   - `\"Chroma is a vector database used for storing embeddings and performing similarity search.\"`\n","   - `\"Document loaders are part of LangChain and help load data from multiple formats.\"`\n","\n","2. Query text: `\"What is Chroma?\"`\n","3. `k=2`\n","\n","### Output:\n","Top `k` results based on similarity:\n","1. Content: `\"Chroma is a vector database used for storing embeddings and performing similarity search.\"`\n","   Metadata: `{...}`\n","2. Content: `\"LangChain is a framework for developing applications powered by LLMs.\"`\n","   Metadata: `{...}`\n","\n","---"],"metadata":{"id":"24KvzOpuYdHC"}},{"cell_type":"code","source":["import chromadb\n","from chromadb.config import Settings\n","from chromadb.utils import embedding_functions\n","from langchain_ollama import OllamaEmbeddings\n","from langchain.document_loaders import TextLoader, PyPDFLoader\n","from langchain.schema import Document\n","from typing import List, Tuple\n","\n","def initialize_chroma(documents: List[Document], db_path: str) -> chromadb.api.Collection:\n","    \"\"\"\n","    Initializes a Chroma vector database and stores documents.\n","\n","    Args:\n","        documents (List[Document]): List of LangChain Document objects.\n","        db_path (str): Path to store the Chroma database.\n","\n","    Returns:\n","        chromadb.api.Collection: Chroma collection object.\n","    \"\"\"\n","    # Initialize Ollama embeddings\n","    ollama_embeddings = OllamaEmbeddings(model=\"llama2\")\n","\n","    # Create a Chroma vector store and store documents\n","    db = chromadb.Client(Settings(chroma_db_impl=\"duckdb+parquet\", persist_directory=db_path))\n","    collection = db.create_collection(name=\"documents\")\n","    collection.add_documents(documents, embedding_function=ollama_embeddings)\n","\n","    return collection\n","\n","\n","def query_database(query: str, db: chromadb.api.Collection, k: int) -> List[str]:\n","    \"\"\"\n","    Queries the Chroma database for the top-k similar documents.\n","\n","    Args:\n","        query (str): Query text.\n","        db (chromadb.api.Collection): Chroma collection object.\n","        k (int): Number of top results to return.\n","\n","    Returns:\n","        List[str]: List of top-k document contents from the database.\n","    \"\"\"\n","    # Perform similarity search in the database\n","    retrieved_results = db.similarity_search(query, k=k)\n","\n","    # Collect the content of the retrieved documents\n","    results = [result.page_content for result in retrieved_results]\n","\n","    return results\n","\n","# Example Usage\n","if __name__ == \"__main__\":\n","    # Define the path to the directory where your documents are stored\n","    directory_path = \"/path/to/your/files\"\n","\n","    # Assuming the load_documents function is already implemented\n","    documents = load_documents(directory_path)\n","\n","    # Initialize Chroma with the documents\n","    db_path = \"/path/to/chroma/db\"\n","    db = initialize_chroma(documents, db_path)\n","\n","    # Define the query and retrieve top-k results\n","    query_text = \"What is Chroma?\"\n","    top_k = 2\n","    results = query_database(query_text, db, top_k)\n","\n","    # Print out the results\n","    for i, result in enumerate(results):\n","        print(f\"Result {i+1}: {result}\")"],"metadata":{"id":"6ujM-v5hYeAP"},"execution_count":null,"outputs":[]}]}